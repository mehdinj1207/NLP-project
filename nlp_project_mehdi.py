# -*- coding: utf-8 -*-
"""NLP_Project_mehdi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15pUd_sLNDPWb2fkDoCaEh8pfFTdtZ4KJ

# I- Importing Modules:
"""

# Commented out IPython magic to ensure Python compatibility.
#Install the required modules
!pip install gensim spacy nltk
!pip install vec2graph
!pip install python-bidi
!pip install arabic-reshaper
!pip install patool
# Importing Libraries
import nltk
import gensim
from gensim.models import FastText
import re
import spacy
import numpy as np
import operator
import pandas as pd
from sklearn.decomposition import PCA
from bidi.algorithm import get_display
import arabic_reshaper
import patoolib
# %matplotlib inline
from matplotlib import pyplot as plt
nltk.download('punkt')
nltk.download("stopwords")
arb_stopwords = set(nltk.corpus.stopwords.words("arabic"))

# Download a model
!wget "https://archive.org/download/aravec_3_wiki/full_grams_cbow_100_wiki.zip"
!unzip "/content/full_grams_cbow_100_wiki.zip"

"""# II- Text Preprocessing:"""

def clean_str(text):
    search = ["أ","إ","آ","ة","_","-","/",".","،"," و "," يا ",'"',"ـ","'","ى","\\",'\n', '\t','&quot;','?','؟','!']
    replace = ["ا","ا","ا","ه"," "," ","","",""," و"," يا","","","","ي","",' ', ' ',' ',' ? ',' ؟ ',' ! ']

    #supprimer tashkeel
    p_tashkeel = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
    text = re.sub(p_tashkeel,"", text)

    #supprimer la  prolongation
    p_longation = re.compile(r'(.)\1+')
    subst = r"\1\1"
    text = re.sub(p_longation, subst, text)

    text = text.replace('وو', 'و')
    text = text.replace('يي', 'ي')
    text = text.replace('اا', 'ا')

    for i in range(0, len(search)):
        text = text.replace(search[i], replace[i])

    text = " ".join([w for w in text.split(" ") if not w in arb_stopwords])

    text = text.strip()

    return text

"""# III- AraVec Model:"""

# Commented out IPython magic to ensure Python compatibility.
# load the AraVec model
model = gensim.models.Word2Vec.load("/content/full_grams_cbow_100_wiki.mdl")
print("We've",len(model.wv.index2word),"vocabularies")
# Export the Word2Vec format + gzip it. make a directory called "spacyModel"
# %mkdir spacyModel
# export the word2vec fomart to the directory
model.wv.save_word2vec_format("./spacyModel/aravec.txt")
# using `gzip` to compress the .txt file
!gzip ./content/spacyModel/aravec.txt

#Initialize the spaCy model using AraVec vectors
#This will create a folder called /spacy.aravec.model within your current working directory

!python -m spacy init vectors ar ./spacyModel/aravec.txt spacy.aravec.model

nlp = spacy.load("./spacy.aravec.model/")

class Preprocessor:
    def __init__(self, tokenizer, **cfg):
        self.tokenizer = tokenizer

    def __call__(self, text):
        preprocessed = clean_str(text)
        return self.tokenizer(preprocessed)
# Appliquer preprocesseur
nlp.tokenizer = Preprocessor(nlp.tokenizer)

"""## Détection des erreurs :"""

def display_pca_scatterplot(model, words=None, sample=0):
    if words == None:
        if sample > 0:
            words = np.random.choice(list(model.vocab.keys()), sample)
        else:
            words = [ word for word in model.vocab ]

    word_vectors = np.array([model[w] for w in words])

    twodim = PCA().fit_transform(word_vectors)[:,:2]
    plt.figure(figsize=(15,10))
    plt.scatter(twodim[:,0]+1, twodim[:,1]-0.1, edgecolors='red', c='orange')

    for word, (x,y) in zip(words, twodim):
      reshaped_text = arabic_reshaper.reshape(u''.join(word))
      artext = get_display(reshaped_text)
      plt.text(x+0.02, y+0.1, artext ,fontsize=20)

# creation du dictionnaire
def creer_dictionnaire(input_text):
  # création des tokens à partir du text
  tokens = np.array(input_text.split(" "))
  dictionary = {}
  for token in tokens:
    token_vector = np.array(nlp(token).vector)  # transformer le vecteur du token a une array
    sum_vec = np.sum(token_vector)  # sommer les valeurs de vecteur
    dictionary[token] = abs(sum_vec)
  return dictionary

# tri décroissant du dictionnaire
def tri_dict(d):
  return sorted(d.items(),key=operator.itemgetter(1),reverse=True)

def calculer_distance(input):
  # input est le dictionnaire trié
  d = input
  s=0
  print(d)
  err = d[0][1]  # valeur sommé
  for i in range(1,len(d)):
    s+=d[i][1]
  center = s/(len(d))
  res = err - center # difference entre l'element et le context général
  return res

def erreur(input,seuil):
  print(input)
  input = clean_str(input)
  dictio = creer_dictionnaire(input)
  print("Dictionary: ",dictio)
  d = tri_dict(dictio)
  distance =  calculer_distance(d)

  errors=list()
  for i in range(0,len(d)):
    d1=list(d)
    d1.remove(d1[i])

    s=0
    for j in range(0,len(d1)):
      s+=d1[j][1]
    corpus_center = s/(len(d1))
    errors.append(d[i][1]-corpus_center)

  ind=errors.index(max(errors))
  word = d[ind][0]

  if errors[ind] >= seuil:
    print("Erreur : ",word)
  else :
    print(" correct : ",word)


  #if visualization==True:
   # display_pca_scatterplot(model, [clean_str(i) for i in input.split(" ")])

  return word

text =  "/الوَلد يَكْتُب الفرس/"
erreur(text,2)

text =  "باضت الدجاجة ثلاث ولد"
erreur(text,2)

text =  " يتعلم سامي الطيران في مسبح المدرسة التي توجد في الحي "
erreur(text,2)

text =  "اخذ العصفور عشه وطار"
erreur(text,2)

df = pd.read_csv('arabic2.txt',sep="\n")
df.columns = ['text']
df

df['Label'] = df['text'].apply(lambda x: x.split("|")[0] )
df['text'] = df['text'].apply(lambda x: x.split("|")[1] )
df

df['Error'] = df['text'].apply(lambda x:erreur(x, 2)  )

df

import numpy as np
from sklearn.metrics import accuracy_score,mean_squared_error,recall_score
accuracy_score(df['Label'],df['Error'])
recall_score(df['Label'],df['Error'],average='micro')

!pip install transformers
!pip install transformers
!pip install farasapy
!pip install pyarabic
!pip install preprocess
!pip install arabert
!git clone github.com/aub-mind/arabert
from transformers import pipeline
from arabert.preprocess import ArabertPreprocessor
MODEL_NAME = 'aubmindlab/bert-base-arabertv02-twitter'

def Substitution(Mot1, Mot2):  # verifier si existe 1 seule substitution
    if len(Mot1) == len(Mot2):
        l = []
        for i in range(len(Mot1)):
            if Mot1[i] == Mot2[i]:
                l.append(0)
            else:
                l.append(1)
        return (sum(l) == 1)
    else:
        return(False)
def Inversion(Mot1, Mot2):
    if len(Mot1) == len(Mot2):  # returns true if exists one and only inversion error
        l1 = []
        l2 = []
        for i in range(len(Mot1)):
            if Mot1[i] == Mot2[i]:
                l1.append(0)
            else:
                l1.append(1)
                l2.append(Mot1[i])
                l2.append(Mot2[i])
        if sum(l1) == 2:
            return(l2[0] == l2[3] and l2[1] == l2[2])
        else:
            return(False)
    else:
        return(False)

def Ajoutsup(Mot1, Mot2):  # ajout/suppression d'un seul caracter
    w1 = set(Mot1)
    w2 = set(Mot2)
    maxi = max(len(w1-w2), len(w2-w1))
    if maxi != 1:
        return(False)
    else:
        if len(w1)-len(w2) == 1:
            lw1 = list(Mot1)
            lw1.remove(''.join(w1-w2))
            s = ''.join(str(x) for x in lw1)
            return(s == Mot2)
        elif len(w1)-len(w2) == -1:
            lw2 = list(Mot2)
            lw2.remove(''.join(w2-w1))
            s = ''.join(str(x) for x in lw2)
            return(s == Mot1)
        else:
            return(False)
def similaire(words, corpustoken):
    for word in words:
        for c in corpustoken:
            if Ajoutsup(word, c):
                return(word)
            elif Inversion(word, c) or Substitution(word, c):  # inversion / substitution
                return(word)
    return('')
def replace(mask, text):
    text = text.split()
    text = list(map(lambda x: x.replace(mask, '[MASK]'), text))
    text = ' '.join(str(x) for x in text)
    return(text)

def analyse(text):
    ok =True
    res=[]
    print("\n","La phrase = ",text,"\n")
    for erreur in text.split():
        erroned_text = replace(erreur,text)
        words = fill_mask(erroned_text)
        s=[]
        t=[]
        for e in words:
    #        t.append(e['token_str'])
            t.append(clean_str(e['token_str']))
            s.append(e['sequence'])
        correct = similaire(t, [clean_str(erreur)]) #les mots parmi les mots générs qui ressemblent au mot à analyser
        if correct != erreur and correct:
            print("Erreur : ", erreur )
            #print("Erreur : ", mask , " || correction : ",correct,"")
            ok = False
            res.append((erreur,correct))
    if ok :
        print("Cette phrase contient 0 erreurs ")
    return(res)
#fill_mask = pipeline(task="fill-mask",model=MODEL_NAME,tokenizer=MODEL_NAME)
#print(fill_mask(" الحياة بسيطة حقا ، لكننا نصر على جعلها [MASK] " ))

def correction(text):
    ok =True
    res=[]
    print("\n","La phrase = ",text,"\n")
    for erreur in text.split():
        erroned_text = replace(erreur,text)
        words = corr(erroned_text)
        s=[]
        t=[]
        for e in words:
    #        t.append(e['token_str'])
            t.append(clean_str(e['token_str']))
            s.append(e['sequence'])
        correct = similaire(t, [clean_str(erreur)]) #les mots parmi les mots générs qui ressemblent au mot à analyser
        if correct != erreur and correct:
            print("Erreur : ", erreur , " || correction : ",correct,"")
            ok = False
            res.append((erreur,correct))
    if ok :
        print("Cette phrase contient 0 erreurs ")
    return(res)

Text1 = ['الحياة بسيطة حقا ، لكننا نصر على جعلها معدة'
,'ارجع البعض السبب إلي ارتفاع اسفار النفط'
,'تعد البطالة احد اهم الظواهر التى تلازم المستمع'
,'لو أعطيت الأحمق خنجرا أصبت قاتلا'
,'لمسة واحدة للضفدع الذهبي السام تسبب الوفاة']

for i in range(0,len(Text1)):
    '/n'
    analyse(Text1[i])
    '/n'

Text1 = ['الحياة بسيطة حقا ، لكننا نصر على جعلها معدة'
,'ارجع البعض السبب إلي ارتفاع اسفار النفط'
,'تعد البطالة احد اهم الظواهر التى تلازم المستمع'
,'لو أعطيت الأحمق خنجرا أصبت قاتلا'
,'لمسة واحدة للضفدع الذهبي السام تسبب الوفاة']

for i in range(0,len(Text1)):
    '/n'
    correction(Text1[i])
    '/n'

def erreur2(input,seuil):
  print(input)
  input = clean_str(input)
  dictio = creer_dictionnaire(input)
  d = tri_dict(dictio)
  distance =  calculer_distance(d)

  errors=list()
  for i in range(0,len(d)):
    d1=list(d)
    d1.remove(d1[i])

    s=0
    for j in range(0,len(d1)):
      s+=d1[j][1]
    corpus_center = s/(len(d1))
    errors.append(d[i][1]-corpus_center)

  ind=errors.index(max(errors))
  word = d[ind][0]

  if errors[ind] >= seuil:
    print("Erreur : ",word)
  else :
    print(" correct : ",word)


  #if visualization==True:
   # display_pca_scatterplot(model, [clean_str(i) for i in input.split(" ")])

  return word

# Reading DataSet
import pandas as pd
df = pd.read_csv('arabic.txt',sep="\n")
df.columns = ['text']
df

df['Label'] = df['text'].apply(lambda x: x.split("|")[0] )
df['text'] = df['text'].apply(lambda x: x.split("|")[1] )
df.head()

df['Error'] = df['text'].apply(lambda x:erreur2(x, 2)  )

true=0
false=0
c=0
for i,j in zip(df['Label'],df['Error']):
  c+=1
  if i == j:
    true+=1
  else:
    false+=1
print("Precision: ",true/c)

